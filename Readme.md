# DeformCAT: Deformable Cross-Attention Transformer for Weakly Aligned RGB-T Pedestrian Detection

Repo for paper "Deformable Cross-Attention Transformer for Weakly Aligned RGB-T Pedestrian Detection", IEEE transactions on Multimedia, by Y. Hu, X. Chen, S. Wang, L. Liu, H. Shi, L. Fan, J. Tian, and J. Liang.

## Preparation

```shell
git clone https://github.com/jiongger/DeformCAT.git
cd ./DeformCAT
```

## Installation

```shell
conda create -n deformcat python=3.8
conda activate deformcat
pip install -r requirements.txt
```

### Datasets

Please download following .zip files and unzip them to ```./datasets``` for your training and evaluating.

#### Aligned-FLIR: 
FLIR-align-3class.zip: [[Google Drive](https://drive.google.com/file/d/1tvS1vDJkKfxhE1F9DPA_hU4_fCIwz-Rk/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxX_WbqwQeLLSAGbw?e=9AWBfK)]

#### CVC-14:
CVC-14.zip: [[Google Drive](https://drive.google.com/file/d/1S1frneQjh7rmR6k2lX-BAhYHyfXBDYbb/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxY2l3mV2pOsKE9xA?e=NlVpQV)]

#### KAIST:
KAIST.zip: [[Google Drive](https://drive.google.com/file/d/16oVUixMBg5nD9cen101n63uz8iG9mvRQ/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxaKpKYdtk-wJ34lQ?e=M41lbw)]

#### LLVIP:
LLVIP.zip: [[Google Drive](https://drive.google.com/file/d/1n4Le8IRxVdjVdIkHMPNp4pBHP9oQRxG2/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxZKnYoYm299exzcw?e=7eIPqb)]

### Weights

We provide pre-trained model for the FLIR, CVC-14, KAIST, and LLVIP datasets.

#### FLIR:
DeformCAT-FLIR.pt [[Google Drive](https://drive.google.com/file/d/1vK2OxUdWEEdimB5_5m2nuA25firHH7Lm/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo0gSlKGKOvdeCXCMQ?e=wqkINJ)]

#### CVC-14:
DeformCAT-CVC.pt: [[Google Drive](https://drive.google.com/file/d/1CVQVMt5z_R_0MhRSBEDZ6x_r0Oa8-RmC/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo0ZNAeduk0fL49Vgg?e=R6Xls4)]

#### KAIST:
DeformCAT-KAIST.pt: [[Google Drive](https://drive.google.com/file/d/14QhME0rOiS63c29U4oFFXKDalX0VtS2_/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo0YFixGiTn0FPG94A?e=NIkv1v)]

#### LLVIP:
DeformCAT-LLVIP.pt: [[Google Drive](https://drive.google.com/file/d/1xM5wEzp0-l12sj7c0TfEOaoS9bz_eQZa/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo07ylGvQcdKjhe33g?e=bELkn3)]

## Train

Use the following command line:
```shell
python train.py --cfg <path-to-network.yaml> --data <path-to-dataset.yaml> --hyp <path-to-hyperparams.yaml> --project <path-to-save> --name <exp_name> --epochs <epochs>
```

Please make sure that the \<name\> dataset has been located in `./datasets/<name>` first. 

After that, if you want to train our model on Aligned-FLIR dataset, use:
```shell
python train.py --cfg ./models/transformer/yolov5l_Transfusion_FLIR_DeformDotAttnLocal.yaml --data ./data/multispectral/FLIR-align-3class.yaml --hyp ./data/hyp.scratch_FLIR.yaml --project saves/FLIR_DeformCrossAttn --name DeformCAT --epochs 15
```

Command lines for training can be found in `./examples.sh`

## Evaluation
Use the following command line to evaluate your trained model:
```shell
python train.py --weights <path-to-save>/<exp_name>/weights/best.pt --data <path-to-dataset.yaml>
```
For example, if you want to evaluate your model trained with the above command line, use:
```shell
python train.py --weights ./saves/FLIR_DeformCrossAttn/DeformCAT/weights/best.pt --data ./data/multispectral/FLIR-align-3class.yaml
```

If you want to evaluate our pre-trained weights for the KAIST dataset, use:
```shell
python test.py --weights ./DeformCAT-KAIST.pt --data ./data/multispectral/KAIST.yaml
```
(assuming that you have put the pre-trained weights `DeformCAT-KAIST.pt` for the KAIST dataset in the base dir, i.e., `./`)

## Results

We have been working on optimizing the hyperparamters for each dataset. The following results are generated by the latest weights and may be better than those reported in our paper. We provided the latest configurations in `./data/hyp.scratch_<dataset>.yaml`.

#### Aligned-FLIR:

|               | P (%)        | R (%)        | AP50 (%)     | AP75 (%)     | AP50:95 (%)  |
|---------------|--------------|--------------|--------------|--------------|--------------|
|     All       |     84.84    |     78.98    |     86.48    |     43.66    |     46.91    |
|     Person    |     85.80    |     81.95    |     89.90    |     45.52    |     47.75    |
|     Car       |     85.46    |     85.69    |     91.86    |     66.35    |     60.38    |
|     Bike      |     83.27    |     69.31    |     77.67    |     19.11    |     32.61    |

#### CVC-14:

|     MR-All     |     MR-Day     |     MR-Night     |
|-------------|-------------|--------------|
|    14.63    |    16.58    |    12.48     |

#### KAIST:

|     MR-All     |     MR-Day     |     MR-Night     |    MR-Near    |     MR-Medium    |     MR-Far      |     MR-None     |     MR-Partial    |     MR-Heavy    |
|-------------|-------------|--------------|-------------|---------------|--------------|--------------|----------------|--------------|
|     6.06    |     8.05    |     2.77     |     0.01    |     12.19     |     33.55    |     19.01    |     22.19      |     45.48    |

#### LLVIP:

|               | P (%)        | R (%)        | AP50 (%)     | AP75 (%)     | AP50:95 (%)  |
|---------------|--------------|--------------|--------------|--------------|--------------|
|     All       |     97.03    |     94.79    |     97.82    |     77.07    |     66.13    |

## Citation
If you find our work useful for your research, please consider citing our paper.

## Acknowledgements
Some of the code is borrowed from [CFT](https://github.com/DocF/multispectral-object-detection) and [ICAFusion](https://github.com/chanchanchan97/ICAFusion). Thanks to their remarkable work!
