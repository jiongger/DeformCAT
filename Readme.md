# DeformCAT: Deformable Cross-Attention Transformer for Weakly Aligned RGB-T Pedestrian Detection

Repo for paper "Deformable Cross-Attention Transformer for Weakly Aligned RGB-T Pedestrian Detection", IEEE transactions on Multimedia, by Y. Hu, X. Chen, S. Wang, L. Liu, H. Shi, L. Fan, J. Tian, and J. Liang.

## Updates
[2025/3/13] - Add results and weights for M3FD (with [zxSplit](https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection)) dataset.

## Preparation

```shell
git clone https://github.com/jiongger/DeformCAT.git
cd ./DeformCAT
```

## Installation

```shell
conda create -n deformcat python=3.8
conda activate deformcat
pip install -r requirements.txt
```

### Datasets

Please download following .zip files and unzip them to ```./datasets``` for your training and evaluating.

#### Aligned-FLIR: 
FLIR-align-3class.zip: [[Google Drive](https://drive.google.com/file/d/1tvS1vDJkKfxhE1F9DPA_hU4_fCIwz-Rk/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxX_WbqwQeLLSAGbw?e=9AWBfK)]

#### CVC-14:
CVC-14.zip: [[Google Drive](https://drive.google.com/file/d/1S1frneQjh7rmR6k2lX-BAhYHyfXBDYbb/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxY2l3mV2pOsKE9xA?e=NlVpQV)]

#### KAIST:
KAIST.zip: [[Google Drive](https://drive.google.com/file/d/16oVUixMBg5nD9cen101n63uz8iG9mvRQ/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxaKpKYdtk-wJ34lQ?e=M41lbw)]

#### LLVIP:
LLVIP.zip: [[Google Drive](https://drive.google.com/file/d/1n4Le8IRxVdjVdIkHMPNp4pBHP9oQRxG2/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3loxZKnYoYm299exzcw?e=7eIPqb)]

#### M3FD:
M3FD.zip: [[OneDrive](https://1drv.ms/u/c/b731757d23096e04/EYK3jqQuFRpBr-72ZYhWLswB3flXgW-LHvVUhYc9_sbISA?e=SyJ9VW)]

### Weights

We provide pre-trained model for the FLIR, CVC-14, KAIST, LLVIP, and M3FD datasets.

#### FLIR:
DeformCAT-FLIR.pt [[Google Drive](https://drive.google.com/file/d/1vK2OxUdWEEdimB5_5m2nuA25firHH7Lm/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo0gSlKGKOvdeCXCMQ?e=wqkINJ)]

#### CVC-14:
DeformCAT-CVC.pt: [[Google Drive](https://drive.google.com/file/d/1CVQVMt5z_R_0MhRSBEDZ6x_r0Oa8-RmC/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo0ZNAeduk0fL49Vgg?e=R6Xls4)]

#### KAIST:
DeformCAT-KAIST.pt: [[Google Drive](https://drive.google.com/file/d/14QhME0rOiS63c29U4oFFXKDalX0VtS2_/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo0YFixGiTn0FPG94A?e=NIkv1v)]

#### LLVIP:
DeformCAT-LLVIP.pt: [[Google Drive](https://drive.google.com/file/d/1xM5wEzp0-l12sj7c0TfEOaoS9bz_eQZa/view?usp=sharing)][[OneDrive](https://1drv.ms/u/s!AgRuCSN9dTG3lo07ylGvQcdKjhe33g?e=bELkn3)]

#### M3FD:
DeformCAT-M3FD.pt: [[OneDrive](https://1drv.ms/u/c/b731757d23096e04/EejDSlL0WhxGq-cK-44BRQsB2ziBlk-NiAp3V4FmsFJsPw?e=Sb4uKx)]

## Train

Use the following command line:
```shell
python train.py --cfg <path-to-network.yaml> --data <path-to-dataset.yaml> --hyp <path-to-hyperparams.yaml> --project <path-to-save> --name <exp_name> --epochs <epochs>
```

Please make sure that the \<name\> dataset has been located in `./datasets/<name>` first. 

After that, if you want to train our model on Aligned-FLIR dataset, use:
```shell
python train.py --cfg ./models/transformer/yolov5l_Transfusion_FLIR_DeformDotAttnLocal.yaml --data ./data/multispectral/FLIR-align-3class.yaml --hyp ./data/hyp.scratch_FLIR.yaml --project saves/FLIR_DeformCrossAttn --name DeformCAT --epochs 15
```

Command lines for training can be found in `./examples.sh`

## Evaluation
Use the following command line to evaluate your trained model:
```shell
python test.py --weights <path-to-save>/<exp_name>/weights/best.pt --data <path-to-dataset.yaml>
```
For example, if you want to evaluate your model trained with the above command line, use:
```shell
python test.py --weights ./saves/FLIR_DeformCrossAttn/DeformCAT/weights/best.pt --data ./data/multispectral/FLIR-align-3class.yaml
```

If you want to evaluate our pre-trained weights for the KAIST dataset, use:
```shell
python test.py --weights ./DeformCAT-KAIST.pt --data ./data/multispectral/KAIST.yaml
```
(assuming that you have put the pre-trained weights `DeformCAT-KAIST.pt` for the KAIST dataset in the base dir, i.e., `./`)

## Results

We have been working on optimizing the hyperparamters for each dataset. The following results are generated by the latest weights and may be better than those reported in our paper. We provided the latest configurations in `./data/hyp.scratch_<dataset>.yaml`.

#### Aligned-FLIR:

|               | P (%)        | R (%)        | AP50 (%)     | AP75 (%)     | AP50:95 (%)  |
|---------------|--------------|--------------|--------------|--------------|--------------|
|     All       |     84.84    |     78.98    |     86.48    |     43.66    |     46.91    |
|     Person    |     85.80    |     81.95    |     89.90    |     45.52    |     47.75    |
|     Car       |     85.46    |     85.69    |     91.86    |     66.35    |     60.38    |
|     Bike      |     83.27    |     69.31    |     77.67    |     19.11    |     32.61    |

#### CVC-14:

|     MR-All     |     MR-Day     |     MR-Night     |
|-------------|-------------|--------------|
|    14.63    |    16.58    |    12.48     |

#### KAIST:

|     MR-All     |     MR-Day     |     MR-Night     |    MR-Near    |     MR-Medium    |     MR-Far      |     MR-None     |     MR-Partial    |     MR-Heavy    |
|-------------|-------------|--------------|-------------|---------------|--------------|--------------|----------------|--------------|
|     6.06    |     8.05    |     2.77     |     0.01    |     12.19     |     33.55    |     19.01    |     22.19      |     45.48    |

#### LLVIP:

|               | P (%)        | R (%)        | AP50 (%)     | AP75 (%)     | AP50:95 (%)  |
|---------------|--------------|--------------|--------------|--------------|--------------|
|     All       |     97.03    |     94.79    |     97.82    |     77.07    |     66.13    |

#### M3FD:

|               | P (%)        | R (%)        | AP50 (%)     | AP75 (%)     | AP50:95 (%)  |
|---------------|--------------|--------------|--------------|--------------|--------------|
|     All       |     84.05    |     66.63    |     73.28    |     49.79    |     46.49    |
|     Person    |     89.45    |     73.31    |     79.95    |     47.20    |     46.16    |
|     Car       |     86.33    |     83.96    |     89.48    |     68.36    |     62.36    |
|     Bus       |     81.73    |     71.17    |     80.23    |     68.72    |     58.45    |
|     Lamp      |     82.86    |     60.17    |     68.13    |     24.81    |     31.14    |
|     Motocycle |     85.29    |     48.73    |     56.26    |     33.94    |     33.43    |
|     Truck     |     76.85    |     62.43    |     65.62    |     55.71    |     47.38    |

Comparison with other -zxSplit baselines:
|                   | AP50 (%)     | AP75 (%)     | AP50:95 (%)  |
|-------------------|--------------|--------------|--------------|
|        EME        |  66.23±0.40  |     -        |  41.10±0.29  |
|       TFDet       |     64.8     |     -        |     41.0     |
|  DeformCAT(ours)  |     73.28    |     49.79    |     46.49    |


## Citation
If you find our work useful for your research, please consider citing our paper.

## Acknowledgements
Some of the code is borrowed from [CFT](https://github.com/DocF/multispectral-object-detection) and [ICAFusion](https://github.com/chanchanchan97/ICAFusion). Thanks to their remarkable work!
